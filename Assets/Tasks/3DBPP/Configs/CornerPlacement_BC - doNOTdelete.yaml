behaviors:
  CornerPlacementAgent:
    trainer_type: ppo

    # Behavioral Cloning - Learn from your demonstrations
    behavioral_cloning:
      demo_path: Assets/Tasks/3DBPP/Demonstrations
      steps: 100000             # Use demos for first 100k steps (more demos = more steps)
      strength: 0.7             # Higher strength since we have more data (0.0-1.0)
      samples_per_update: 512   # Samples from demos per update

    # PPO Hyperparameters
    hyperparameters:
      learning_rate: 3.0e-4
      learning_rate_schedule: linear

      batch_size: 1024
      buffer_size: 10240

      beta: 5.0e-3              # Entropy bonus
      epsilon: 0.2              # PPO clip range
      lambd: 0.95               # GAE lambda
      num_epoch: 3              # Epochs per update

    # Network Architecture
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple

    # Reward signals
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    # Training settings
    max_steps: 200000           # Total training steps
    time_horizon: 64            # Steps before trajectory complete
    summary_freq: 10000         # TensorBoard update frequency

    # Other settings
    keep_checkpoints: 5
    checkpoint_interval: 50000
    threaded: false
